<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="https://pranavra0.github.io/images/favicon.png" />
<title>Project4 | ...</title>
<meta name="title" content="Project4" />
<meta name="description" content="Introduction
In this project, I explore the chemical composition of various wines using unsupervised learning techniques, specifically clustering. The core idea is to group wines into meaningful clusters based solely on their chemical properties.
The goal is to uncover natural groupings in the dataset and answer key questions: Can we separate wines into distinct types based on their chemical profiles? Which features contribute most to those differences? And how many wine clusters make sense from a data-driven perspective?" />
<meta name="keywords" content="" />


<meta property="og:url" content="https://pranavra0.github.io/project4/">
  <meta property="og:site_name" content="...">
  <meta property="og:title" content="Project4">
  <meta property="og:description" content="Introduction In this project, I explore the chemical composition of various wines using unsupervised learning techniques, specifically clustering. The core idea is to group wines into meaningful clusters based solely on their chemical properties.
The goal is to uncover natural groupings in the dataset and answer key questions: Can we separate wines into distinct types based on their chemical profiles? Which features contribute most to those differences? And how many wine clusters make sense from a data-driven perspective?">
  <meta property="og:locale" content="en_US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2025-04-11T16:51:24-04:00">
    <meta property="article:modified_time" content="2025-04-11T16:51:24-04:00">




  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Project4">
  <meta name="twitter:description" content="Introduction In this project, I explore the chemical composition of various wines using unsupervised learning techniques, specifically clustering. The core idea is to group wines into meaningful clusters based solely on their chemical properties.
The goal is to uncover natural groupings in the dataset and answer key questions: Can we separate wines into distinct types based on their chemical profiles? Which features contribute most to those differences? And how many wine clusters make sense from a data-driven perspective?">




  <meta itemprop="name" content="Project4">
  <meta itemprop="description" content="Introduction In this project, I explore the chemical composition of various wines using unsupervised learning techniques, specifically clustering. The core idea is to group wines into meaningful clusters based solely on their chemical properties.
The goal is to uncover natural groupings in the dataset and answer key questions: Can we separate wines into distinct types based on their chemical profiles? Which features contribute most to those differences? And how many wine clusters make sense from a data-driven perspective?">
  <meta itemprop="datePublished" content="2025-04-11T16:51:24-04:00">
  <meta itemprop="dateModified" content="2025-04-11T16:51:24-04:00">
  <meta itemprop="wordCount" content="915">
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  :root {
      --width: 800px;
      --font-main: Times New Roman, serif;
      --font-secondary: Times, serif;
      --font-scale: 1.1em;
      --background-color: #fff;
      --heading-color: #222;
      --text-color: #444;
      --link-color: #3273dc;
      --visited-color:  #8b6fcb;
      --code-background-color: #09090b;
      --code-color: #f8f8f2;
      --blockquote-color: #222;
  }

  @media (prefers-color-scheme: dark) {
    :root {
        --background-color: #09090b;
        --heading-color: #ffffff;
        --text-color: #cccccc;
        --link-color: #7abfff;
        --visited-color: #b693f9;
        --code-background-color: #2e2e3e;
        --code-color: #f8f8f2;
        --blockquote-color: #bbbbbb;
    }
}

  body {
      font-family: var(--font-secondary);
      font-size: var(--font-scale);
      margin: auto;
      padding: 20px;
      max-width: var(--width);
      text-align: left;
      background-color: var(--background-color);
      word-wrap: break-word;
      overflow-wrap: break-word;
      line-height: 1.5;
      color: var(--text-color);
  }

  h1, h2, h3, h4, h5, h6 {
      font-family: var(--font-main);
      color: var(--heading-color);
      font-style: italic;   
  }

  a {
      color: var(--link-color);
      cursor: pointer;
      text-decoration: none;
  }

  a:hover {
      text-decoration: underline; 
  }

  nav a {
      margin-right: 8px;
  }

  strong, b {
      color: var(--heading-color);
  }

  button {
      margin: 0;
      cursor: pointer;
  }

  main {
      line-height: 1.6;
  }

  table {
      width: 100%;
  }

  hr {
      border: 0;
      border-top: 1px dashed;
  }

  img {
      max-width: 100%;
  }

  code {
    font-family: 'Fira Code', monospace;
    background-color: var(--code-background-color);
    color: var(--code-color);
    border-radius: 6px;
    font-size: 0.95em;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.5);
    padding: 0;  
    margin: 0px;  
    outline: 2px dashed #999;  
    outline-offset: 4px;  
}


  blockquote {
      border-left: 1px solid #999;
      color: var(--blockquote-color);
      padding-left: 20px;
      font-style: italic;
  }

  footer {
      padding: 25px 0;
      text-align: center;
  }

  .title:hover {
      text-decoration: none;
  }

  .title h1 {
      font-size: 1.5em;
  }

  .inline {
      width: auto !important;
  }

    .highlight .keyword, .code .keyword {
        color: #ff79c6;
        font-weight: bold;
    }

    .highlight .string, .code .string {
        color: #f1fa8c;
    }

    .highlight .comment, .code .comment {
        color: #6272a4;
        font-style: italic;
    }

    .highlight .number, .code .number {
        color: #bd93f9;
    }

   
  ul.blog-posts {
      list-style-type: none;
      padding: unset;
  }

  ul.blog-posts li {
      display: flex;
  }

  ul.blog-posts li span {
      flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
      color: var(--visited-color);
  }
</style>

</head>

<body>
  <header><a href="/" class="title">
  <h2>...</h2>
</a>
<nav><a href="/">Home</a>


<a href="/blog">Blog</a>

</nav>
</header>
  <main>

<h1>Project4</h1>
<p>
  <i>
    <time datetime='2025-04-11'>
      2025-04-11
    </time>
  </i>
</p>

<content>
  <h1 id="introduction">Introduction</h1>
<p>In this project, I explore the chemical composition of various wines using unsupervised learning techniques, specifically clustering. The core idea is to group wines into meaningful clusters based solely on their chemical properties.</p>
<p>The goal is to uncover natural groupings in the dataset and answer key questions: Can we separate wines into distinct types based on their chemical profiles? Which features contribute most to those differences? And how many wine clusters make sense from a data-driven perspective?</p>
<h1 id="what-is-clustering">What is clustering?</h1>
<p>Clustering is a type of unsupervised learning that automatically groups data points based on similarity. The goal is to find natural patterns or structures in the data, where items in the same group (or cluster) are more similar to each other than to items in other groups.</p>
<h2 id="k-means">K-Means</h2>
<ol>
<li>
<p>Pick k: Choose the number of clusters you want to find.</p>
</li>
<li>
<p>Initialize: Randomly place k cluster centers (called centroids) in the feature space.</p>
</li>
<li>
<p>Assign Points: Each data point is assigned to the nearest centroid.</p>
</li>
<li>
<p>Update Centers: Recalculate the centroids as the mean of all points in each cluster.</p>
</li>
<li>
<p>Repeat: Steps 3 and 4 repeat until the assignments stop changing or a maximum number of iterations is reached.</p>
</li>
</ol>
<h2 id="agglomerative">Agglomerative</h2>
<ol>
<li>
<p>Start with each point as its own cluster</p>
</li>
<li>
<p>At each step, merge the 2 closests clusters</p>
</li>
<li>
<p>Continue until all the points are grouped into one big cluster</p>
</li>
</ol>
<h2 id="the-dataset">The Dataset</h2>
<p>The dataset includes the following features:</p>
<ul>
<li>Alcohol</li>
<li>Malic acid</li>
<li>Ash</li>
<li>Alcalinity of ash</li>
<li>Magnesium</li>
<li>Total phenols</li>
<li>Flavanoids</li>
<li>Nonflavanoid phenols</li>
<li>Proanthocyanins</li>
<li>Color intensity</li>
<li>Hue</li>
<li>OD280/OD315 of diluted wines</li>
<li>Proline</li>
</ul>
<p>What half of these mean? I have 0 clue and that is why unsupervised learning is so useful.</p>
<p>Immediately upon loading the data I noted that there were absolutely 0 null values meaning we have absolutely nothing to do as far as preprocessing goes. So how does the data look?</p>
<p><img src="/images/project4/ash_dist.png" alt="ash"></p>
<p><img src="/images/project4/alcohol_dist.png" alt="alc"></p>
<p><img src="/images/project4/malic_v_ash.png" alt="malic"></p>
<p>We can already see that there seems to be some type of relationship between malic acid and ash. We also can see that for features like Ash and Alcohol, there is a fairly normal distribution. I could show more but that would be a lot of images to embed. Nearly all the other features are distributed in this fashion, for the record.</p>
<h1 id="preprocessing">Preprocessing</h1>
<p>Honestly? We don&rsquo;t really have much at all to do. The data doesn&rsquo;t contain anything categorial and is purely numerical. Additionally, there are no null values to take care of. We can go straight into actually running our models and seeing what clusters we end up with.</p>
<p>The one bit of preprocessing I will do is scale the dataset to ensure that each feature contributes equally to the distance calculations.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>scaler <span style="color:#f92672">=</span> StandardScaler()
</span></span><span style="display:flex;"><span>scaled_data <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>fit_transform(df)
</span></span></code></pre></div><h1 id="modelling">Modelling</h1>
<p>Before modelling, what clustering method should we use? I am writing this on a 5 year old Thinkpad so I mam leaning towards K-Means. It is much more computationally efficient compared to Agglomerative. Agglomerative Clustering typically runs in O(n^2) or O(n^3).</p>
<p>Additionally, the fixed number of clusters will be far easier to interpret once we get to that point of the process. The final reason is that clusters in K-Means are approximately spherical. My guess is that we will have some fairly seperated groupings and K-Means will accentuate that more than agglomerative.</p>
<p>As far as finding out how many clusters I will need, I will simply use the elbow method.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>inertia <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>K <span style="color:#f92672">=</span> range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">11</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> K:
</span></span><span style="display:flex;"><span>    kmeans <span style="color:#f92672">=</span> KMeans(n_clusters<span style="color:#f92672">=</span>k, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>    kmeans<span style="color:#f92672">.</span>fit(scaled_data)
</span></span><span style="display:flex;"><span>    inertia<span style="color:#f92672">.</span>append(kmeans<span style="color:#f92672">.</span>inertia_)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(K, inertia, <span style="color:#e6db74">&#39;bo-&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Number of clusters (k)&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Inertia&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Elbow Method For Optimal k&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img src="/images/project4/elbow.png" alt=""></p>
<p>Given this I think 3 is a pretty reasonable choice to be our K for modelling.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>kmeans <span style="color:#f92672">=</span> KMeans(n_clusters<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>clusters <span style="color:#f92672">=</span> kmeans<span style="color:#f92672">.</span>fit_predict(scaled_data)
</span></span></code></pre></div><p>using this code I append a Cluster column to my original dataframe</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>kmeans <span style="color:#f92672">=</span> KMeans(n_clusters<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">42</span>)
</span></span><span style="display:flex;"><span>clusters <span style="color:#f92672">=</span> kmeans<span style="color:#f92672">.</span>fit_predict(scaled_data)
</span></span></code></pre></div><p>Let&rsquo;s look at how our data looks using a PCA projection!</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.decomposition <span style="color:#f92672">import</span> PCA
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>pca <span style="color:#f92672">=</span> PCA(n_components<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>reduced_data <span style="color:#f92672">=</span> pca<span style="color:#f92672">.</span>fit_transform(scaled_data)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(reduced_data[:, <span style="color:#ae81ff">0</span>], reduced_data[:, <span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span>clusters, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;viridis&#39;</span>, s<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;KMeans Clustering (2D PCA Projection)&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Principal Component 1&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Principal Component 2&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img src="/images/project4/pca.png" alt=""></p>
<p>And what about the importance of specific features?</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>cluster_centers <span style="color:#f92672">=</span> scaler<span style="color:#f92672">.</span>inverse_transform(kmeans<span style="color:#f92672">.</span>cluster_centers_)
</span></span><span style="display:flex;"><span>features <span style="color:#f92672">=</span> df<span style="color:#f92672">.</span>columns[:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]  <span style="color:#75715e"># don&#39;t touch ... excludes cluster column</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">6</span>))
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(kmeans<span style="color:#f92672">.</span>n_clusters):
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(features, cluster_centers[i], marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;o&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Cluster </span><span style="color:#e6db74">{</span>i<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xticks(rotation<span style="color:#f92672">=</span><span style="color:#ae81ff">45</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;Cluster Centers Across Features&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;Features&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;Original Feature Scale&#39;</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>grid(<span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p><img src="/images/project4/import.png" alt=""></p>
<p>Proline is really important! And a quick google search tells us that yeast uses proline to produce alcohol during fermentation. This has a big impact on how the final product ends up being.</p>
<h1 id="conclusion">Conclusion</h1>
<p>This project can positively impact the wine industry by providing valuable insights into wine types based on chemical properties, improving recommendations, quality control, and marketing strategies. It could also help consumers make more informed choices by grouping wines with similar characteristics, enhancing their overall buying experience. However, there are potential negative impacts to consider. Clustering based solely on chemical data might oversimplify wine classification, ignoring the complexity of human taste perceptions and subtle nuances that aren&rsquo;t captured in the data. Additionally, if the dataset is biased or unrepresentative, it could lead to skewed results that favor certain wine types or regions, reinforcing existing market imbalances. There&rsquo;s also the risk of overrelying on algorithms, which could marginalize less popular wine varieties or create unfair competition by prioritizing the more commercially successful clusters. These issues highlight the need for careful, critical thinking when applying data-driven methods in the wine industry, ensuring that the results are used ethically and that all wine types are fairly represented.</p>

</content>
<p>
  
</p>

  </main>
  <footer>
</footer>

    
</body>

</html>
